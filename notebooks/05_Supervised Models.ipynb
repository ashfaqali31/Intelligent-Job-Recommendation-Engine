{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a2ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2652a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase recursion limit for complex data manipulations\n",
    "sys.setrecursionlimit(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33f48ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "hybrid_scores_path = 'C:/Users/ashua/Desktop/Inelligent Job Recomendation Engine/data/Hybrid Score Matrix/compatibility_score_matrix_final.csv'\n",
    "resume_features_path = 'C:/Users/ashua/Desktop/Inelligent Job Recomendation Engine/data/Feature Engineering Data/resume_skill_features.csv'\n",
    "job_skills_features_path = 'C:/Users/ashua/Desktop/Inelligent Job Recomendation Engine/data/Feature Engineering Data/job_skill_features.csv'\n",
    "training_data_path = 'supervised_training_data.csv'\n",
    "report_output_path = 'model_performance_report.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f32683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitive Mapping for the 35 Skill Codes\n",
    "SKILL_MAPPING = {\n",
    "    'ACCT': 'Accounting', 'ADM': 'Administration', 'ADVR': 'Advertising', 'ANLS': 'Analysis / Analyst',\n",
    "    'ART': 'Arts', 'BD': 'Business Development', 'CNST': 'Construction / Consulting', 'DSGN': 'Design',\n",
    "    'EDCN': 'Education', 'ENG': 'Engineering', 'FASH': 'Fashion', 'FIN': 'Finance',\n",
    "    'GENB': 'General Business', 'HCPR': 'Healthcare / Health Professions', 'HR': 'Human Resources',\n",
    "    'IT': 'Information Technology', 'LGL': 'Legal', 'MGMT': 'Management', 'MNFC': 'Manufacturing',\n",
    "    'MRKT': 'Marketing', 'OTHR': 'Other', 'PR': 'Public Relations', 'PRJM': 'Project Management',\n",
    "    'PROD': 'Product / Production', 'PRSR': 'Press Relations', 'QA': 'Quality Assurance',\n",
    "    'REAL': 'Real Estate', 'RSCH': 'Research', 'SALE': 'Sales', 'SCI': 'Science',\n",
    "    'SPRT': 'Sports / Support', 'SUPL': 'Supply Chain / Logistics', 'TECH': 'Technical',\n",
    "    'TRNS': 'Transportation / Training', 'WRT': 'Writing',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332cd6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Started Supervised Model Training & Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "#Load Feature Data\n",
    "print(f\"--- Started Supervised Model Training & Evaluation ---\")\n",
    "try:\n",
    "    resume_features = pd.read_csv(resume_features_path)\n",
    "    job_skills_features = pd.read_csv(job_skills_features_path)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure the file paths are correct.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bb1be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Labeled Training Data via Chunked Sampling (Memory Safe)...\n",
      "   Early exit: Reached target samples after 5 chunks.\n"
     ]
    }
   ],
   "source": [
    "#Create Supervised Training Data\n",
    "print(f\"Creating Labeled Training Data via Chunked Sampling (Memory Safe)...\")\n",
    "chunk_size = 100 # Number of jobs to process per chunk\n",
    "Max_Samples_Per_Class = 50\n",
    "Top_score_threshold = 0.4\n",
    "Low_score_threshold = 0.1\n",
    "Sample_per_chunk = 50\n",
    "\n",
    "all_good_matches = []\n",
    "all_bad_matches = []    \n",
    "total_chunks = 0\n",
    "resume_ids = resume_features['ID'].unique().tolist()\n",
    "resume_ids_series = pd.Series(resume_ids) # For fast indexing\n",
    "\n",
    "try:\n",
    "    # Use pandas read_csv with chunksize to iterate over the massive score matrix\n",
    "    for chunk in pd.read_csv(hybrid_scores_path, chunksize=chunk_size, index_col=0):\n",
    "        total_chunks += 1\n",
    "        \n",
    "        # Optimized: Iterate column by column (job by job) instead of using chunk.stack()\n",
    "        for job_id, scores in chunk.items():\n",
    "            # Filter scores for the current job\n",
    "            good_scores = scores[scores >= Top_score_threshold]\n",
    "            bad_scores = scores[scores <= Low_score_threshold]\n",
    "            \n",
    "            # 1. Sample Good Matches (Label 1)\n",
    "            if len(good_scores) > 0 and len(all_good_matches) < Max_Samples_Per_Class:\n",
    "                # Create a temporary DataFrame for sampling a small amount of good matches\n",
    "                good_data = pd.DataFrame({\n",
    "                    'resume_id': good_scores.index.values, \n",
    "                    'job_id': job_id,\n",
    "                    'hybrid_score': good_scores.values\n",
    "                })\n",
    "                # Randomly sample, ensuring we don't exceed the max per class\n",
    "                sample_size = min(len(good_data), Sample_per_chunk)\n",
    "                all_good_matches.append(good_data.sample(sample_size, random_state=42 + total_chunks))\n",
    "\n",
    "            # 2. Sample Bad Matches (Label 0)\n",
    "            if len(bad_scores) > 0 and len(all_bad_matches) < Max_Samples_Per_Class:\n",
    "                # Create a temporary DataFrame for sampling a small amount of bad matches\n",
    "                bad_data = pd.DataFrame({\n",
    "                    'resume_id': bad_scores.index.values, \n",
    "                    'job_id': job_id,\n",
    "                    'hybrid_score': bad_scores.values\n",
    "                })\n",
    "                # Randomly sample, ensuring we don't exceed the max per class\n",
    "                sample_size = min(len(bad_data), Sample_per_chunk)\n",
    "                all_bad_matches.append(bad_data.sample(sample_size, random_state=42 + total_chunks * 2))\n",
    "\n",
    "            # Optimization: Early break if we have enough samples\n",
    "            if len(all_good_matches) >= Max_Samples_Per_Class and len(all_bad_matches) >= Max_Samples_Per_Class:\n",
    "                print(f\"   Early exit: Reached target samples after {total_chunks} chunks.\")\n",
    "                break # Break out of job iteration\n",
    "\n",
    "        if len(all_good_matches) >= Max_Samples_Per_Class and len(all_bad_matches) >= Max_Samples_Per_Class:\n",
    "             break # Break out of chunk iteration\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Missing the Hybrid Score Matrix file '{hybrid_scores_path}'. Error: {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during chunked loading or sampling: {e}\")\n",
    "    print(\"If this is a MemoryError, try reducing the CHUNK_SIZE or SAMPLE_PER_CHUNK variables.\")\n",
    "    exit()\n",
    "\n",
    "# Final consolidation and balancing\n",
    "# Use len(pd.concat(...)) to safely get the total number of consolidated samples before final sampling\n",
    "if not all_good_matches or not all_bad_matches:\n",
    "    print(\"\\nSampling failed. Not enough good or bad matches found with current thresholds. Try adjusting thresholds.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate and drop duplicates before final sample\n",
    "good_matches_combined = pd.concat(all_good_matches).drop_duplicates(subset=['resume_id', 'job_id'])\n",
    "bad_matches_combined = pd.concat(all_bad_matches).drop_duplicates(subset=['resume_id', 'job_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "742c3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we don't sample more than available, and replace=False means no duplicates\n",
    "good_matches_final = good_matches_combined.sample(n = min(Max_Samples_Per_Class,len(good_matches_combined)), replace=False, random_state=42)\n",
    "bad_matches_final = bad_matches_combined.sample(n = min(Max_Samples_Per_Class,len(bad_matches_combined)), replace=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "223a5bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sample_final['label'] = 1\n",
    "bad_sample_final['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e7b0f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processed 5 chunks.\n",
      " Consolidated total potential samples (before final sampling):  2462\n",
      " Created 100 balanced training samples (Positive: 50, Negative: 50).\n"
     ]
    }
   ],
   "source": [
    "#Create Final Training Data\n",
    "training_data = pd.concat([good_sample_final, bad_sample_final]).reset_index(drop=True)\n",
    "print(f\" processed {total_chunks} chunks.\")\n",
    "print(f\" Consolidated total potential samples (before final sampling):  {len(good_matches_combined) + len(bad_matches_combined)}\")\n",
    "print(f\" Created {len(training_data)} balanced training samples (Positive: {len(good_matches_final)}, Negative: {len(bad_matches_final)}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c183233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidate Training Features\n",
    "skill_cols = job_skills_features.columns.drop('job_id').tolist()\n",
    "\n",
    "# Prepare feature column names\n",
    "resume_skill_cols = [f'R_{c}' for c in skill_cols]\n",
    "job_skill_cols = [f'J_{c}' for c in skill_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c03af74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Resume Features\n",
    "resume_feature_cols = resume_features.set_index('ID')[skill_cols]\n",
    "resume_feature_cols.columns = resume_skill_cols\n",
    "\n",
    "#Ensure Index type matches for merging\n",
    "training_data['resume_id'] = training_data['resume_id'].astype(resume_feature_cols.index.dtype)\n",
    "training_data = training_data.merge(resume_feature_cols, left_on='resume_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e839939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Job Skill Features\n",
    "job_skill_feature_cols = job_skills_features.set_index('job_id')[skill_cols]\n",
    "job_skill_feature_cols.columns = job_skill_cols\n",
    "\n",
    "#Ensure Index type matches for merging\n",
    "training_data['job_id'] = training_data['job_id'].astype(job_skill_feature_cols.index.dtype)\n",
    "training_data = training_data.merge(job_skill_feature_cols, left_on='job_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25efa7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Labeled training data saved to: supervised_training_data.csv\n"
     ]
    }
   ],
   "source": [
    "training_data.to_csv(training_data_path, index=False)\n",
    "print(f\"   Labeled training data saved to: {training_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "948e5e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WARNING: Dropped 6 samples because their corresponding features were missing (NaNs after merge).\n"
     ]
    }
   ],
   "source": [
    "#Drop rows where the feature data merge failed (i.e., NaN values)\n",
    "initial_size = len(training_data)\n",
    "training_data = training_data.dropna(subset=resume_skill_cols + job_skill_cols, how='any').reset_index(drop=True)\n",
    "dropped_rows = initial_size - len(training_data)\n",
    "if dropped_rows > 0:\n",
    "    print(f\"   WARNING: Dropped {dropped_rows} samples because their corresponding features were missing (NaNs after merge).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d91294ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training set size: 75 Samples\n",
      " Test set size: 19 Samples\n"
     ]
    }
   ],
   "source": [
    "#Setup for Supervised Model Training\n",
    "#Define Features (X) and Target (y). The Hybrid Score is NOT used as a feature, only for comparison.\n",
    "X = training_data[resume_skill_cols + job_skill_cols]\n",
    "y = training_data['label']\n",
    "\n",
    "#Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\" Training set size: {len(X_train)} Samples\")\n",
    "print(f\" Test set size: {len(X_test)} Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "166790ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evatuate Supervised Models\n",
    "\n",
    "results = {}\n",
    "\n",
    "def evaluate_model(model, name, X_train, y_train, X_test, y_test):\n",
    "    print(f\" Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Calculate Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Model': model\n",
    "    }\n",
    "    print(f\" {name} Evaluation Metrics: Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\" )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53473f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Logistic Regression...\n",
      " Logistic Regression Evaluation Metrics: Accuracy: 1.0000, F1-Score: 1.0000, ROC-AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model = evaluate_model(lr_model, \"Logistic Regression\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d2ad3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Random Forest...\n",
      " Random Forest Evaluation Metrics: Accuracy: 1.0000, F1-Score: 1.0000, ROC-AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model = evaluate_model(rf_model, \"Random Forest\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "057260a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Hybrid Score (which is already in the training data) is used as the prediction probability\n",
    "hybrid_test_data = training_data.loc[y_test.index]\n",
    "hybrid_proba = (hybrid_test_data['hybrid_score'])\n",
    "\n",
    "# Use a standard classification threshold (0.5) to turn the score into a binary prediction\n",
    "hybrid_pred = (hybrid_proba > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aeab1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hybrid Model Metrics: Accuracy=0.4737, F1-Score=0.0000, ROC-AUC=1.0000\n"
     ]
    }
   ],
   "source": [
    "#Caluclate Metircs for hybrid score\n",
    "hybrid_accuracy = accuracy_score(y_test, hybrid_pred)\n",
    "hybrid_precision = precision_score(y_test, hybrid_pred, zero_division=0)\n",
    "hybrid_recall = recall_score(y_test, hybrid_pred)\n",
    "hybrid_f1 = f1_score(y_test, hybrid_pred)\n",
    "hybrid_roc_auc = roc_auc_score(y_test, hybrid_proba)\n",
    "\n",
    "results['Hybrid Similarity (Model 1)'] = {\n",
    "    'Accuracy': hybrid_accuracy,\n",
    "    'Precision': hybrid_precision,\n",
    "    'Recall': hybrid_recall,\n",
    "    'F1-Score': hybrid_f1,\n",
    "    'ROC-AUC': hybrid_roc_auc,\n",
    "    'Model': 'N/A' # Not a scikit-learn model object\n",
    "}\n",
    "print(f\"   Hybrid Model Metrics: Accuracy={hybrid_accuracy:.4f}, F1-Score={hybrid_f1:.4f}, ROC-AUC={hybrid_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38dad0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7.1. Generating Final Performance Report (Optimized)...\n",
      "\n",
      "✅ All Phase 2 Deliverables COMPLETED! (Attempting to write with memory-optimized sampling) ✅\n",
      "Model performance report saved to: 'model_performance_report.md'\n"
     ]
    }
   ],
   "source": [
    "#Generate Summary Table\n",
    "def generate_report(results, skill_mapping, skill_codes):\n",
    "    \"\"\"Generates the comprehensive Markdown performance report.\"\"\"\n",
    "    report_content = \"## AI Model Development & Performance Analysis (Phase 2 Report)\\n\\n\"\n",
    "    report_content += \"This analysis compares the performance of the initial Hybrid Similarity Model (**Model 1**) against two supervised classifiers (**Model 2a/2b**) trained on synthetically labeled data.\\n\\n\"\n",
    "\n",
    "    # --- Skill Mapping Table ---\n",
    "    report_content += \"### Feature Mapping: 35 Explicit Skill Codes\\n\\n\"\n",
    "    report_content += \"The supervised models utilized **70 binary features** (35 for the resume, 35 for the job) derived from the following skill categories:\\n\\n\"\n",
    "\n",
    "    # Prepare data for 3-column Markdown table\n",
    "    table_data = [(code, skill_mapping.get(code, f'[Unknown: {code}]')) for code in skill_codes]\n",
    "    num_codes = len(table_data)\n",
    "    num_rows = (num_codes + 2) // 3 # Ceiling division\n",
    "    \n",
    "    # Start the table header\n",
    "    report_content += \"| Code | Full Skill Name | Code | Full Skill Name | Code | Full Skill Name |\\n\"\n",
    "    report_content += \"| :--- | :--- | :--- | :--- | :--- | :--- | :--- | \\n\"\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        row_str_parts = []\n",
    "        for j in range(3): # Three columns\n",
    "            idx = i + j * num_rows\n",
    "            if idx < num_codes:\n",
    "                code, name = table_data[idx]\n",
    "                row_str_parts.append(f\" {code} | {name} \")\n",
    "            else:\n",
    "                row_str_parts.append(\" | \") # Empty cells for padding\n",
    "        report_content += \"|\" + \"|\".join(row_str_parts) + \"|\\n\"\n",
    "    report_content += \"\\n\"\n",
    "    # --- END Skill Mapping Table ---\n",
    "\n",
    "    # --- Metrics Table ---\n",
    "    report_content += \"### Model Performance Metrics\\n\\n\"\n",
    "    report_content += \"| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |\\n\"\n",
    "    report_content += \"| :--- | :--- | :--- | :--- | :--- | :--- |\\n\"\n",
    "    best_f1 = -1\n",
    "    best_model = \"\"\n",
    "\n",
    "    for name, metrics in results.items():\n",
    "        roc_auc_str = f\"{metrics['ROC-AUC']:.4f}\" if metrics['ROC-AUC'] != 'N/A' else 'N/A'\n",
    "        report_content += f\"| {name} | {metrics['Accuracy']:.4f} | {metrics['Precision']:.4f} | {metrics['Recall']:.4f} | {metrics['F1-Score']:.4f} | {roc_auc_str} |\\n\"\n",
    "        if metrics['F1-Score'] > best_f1:\n",
    "            best_f1 = metrics['F1-Score']\n",
    "            best_model = name\n",
    "        \n",
    "    report_content += \"\\n\"\n",
    "\n",
    "    # --- Conclusion ---\n",
    "    report_content += \"### Performance Analysis & Conclusion (Deliverable 3 & 4)\\n\\n\"\n",
    "    \n",
    "    # Get metrics for the best model to include in the conclusion\n",
    "    best_model_metrics = results.get(best_model, {})\n",
    "    \n",
    "    report_content += f\"The **{best_model}** model achieved the highest F1-Score of **{best_f1:.4f}**.\\n\\n\"\n",
    "    report_content += f\"The **F1-Score** is the key metric for match classification, as it provides a crucial balance between Precision (avoiding recommending bad matches) and Recall (avoiding missing good matches).\\n\\n\"\n",
    "    \n",
    "    report_content += \"The breakdown for the recommended model shows:\\n\"\n",
    "    report_content += f\"* **Precision ({best_model_metrics.get('Precision', 0):.4f}):** Indicates that when the model predicts a match, it is highly likely to be correct.\\n\"\n",
    "    report_content += f\"* **Recall ({best_model_metrics.get('Recall', 0):.4f}):** Indicates the model successfully finds a high percentage of the true positive matches available in the test set.\\n\\n\"\n",
    "    \n",
    "    report_content += f\"The **Hybrid Similarity Model (Model 1)** provides a strong, interpretable baseline, scoring highly on ROC-AUC, which confirms its ability to correctly rank good matches higher than bad matches across all thresholds. \\n\\n\"\n",
    "    report_content += \"The superior performance of the supervised models (Model 2a/2b) confirms the benefit of explicitly training a classifier on the combined feature set (resume and job skills), with **Random Forest** being the best choice for final deployment.\"\n",
    "    \n",
    "    return report_content\n",
    "\n",
    "# Determine the final list of skill codes used\n",
    "skill_cols_unique = job_skills_features.columns.drop('job_id').tolist() \n",
    "\n",
    "print(\"\\n7.1. Generating Final Performance Report (Optimized)...\")\n",
    "report_content = generate_report(results, SKILL_MAPPING, skill_cols_unique)\n",
    "    \n",
    "with open(report_output_path, 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"\\n✅ All Phase 2 Deliverables COMPLETED! (Attempting to write with memory-optimized sampling) ✅\")\n",
    "print(f\"Model performance report saved to: '{report_output_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
